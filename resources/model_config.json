{
    "$schema": "./ai-agents.schema.json",
    "sorting": [
        "calm3-22b-chat",
        "Meta-Llama-3-8B-Instruct",
        "gemma-2-27b-it",
        "Llama-3.2-11B-Vision-Instruct",
        "stable-diffusion-3-medium",
        "Talkativot UI"
    ],
    "models": [
    {
        "modelName": "stable-diffusion-3-medium",
        "shortName": "stable-diffusion-3m",
        "environments": {
            "registry": "cr.backend.ai",
            "name": "testing/ngc-pytorch",
            "tag": "24.07-pytorch2.4-py310-cuda12.5",
            "architecture": "x86_64",
            "environment": "",
            "version": "1.0.0"
        },
        "runtimeVariant": "custom",
        "cluster_size": 1,
        "cluster_mode": "single-node",
        "openToPublic": true,
        "resource": {
            "cpu": 4,
            "mem": "32g",
            "accelerator": "10",
            "acceleratorType": "cuda.shares",
            "shmem": "1g"
        },
        "envvars": [],
        "enabledAutomaticShmem": false
    },
    {
        "modelName": "Llama-3.2-11B-Vision-Instruct",
        "shortName": "llama-vision-11b",
        "environments": {
            "registry": "cr.backend.ai",
            "name": "testing/vllm",
            "tag": "0.6.6-cuda12.4-ubuntu22.04",
            "architecture": "x86_64",
            "environment": "",
            "version": "1.0.0"
        },
        "runtimeVariant": "vllm",
        "cluster_size": 1,
        "cluster_mode": "single-node",
        "openToPublic": true,
        "resource": {
            "cpu": 4,
            "mem": "32g",
            "accelerator": "10",
            "acceleratorType": "cuda.shares",
            "shmem": "1g"
        },
        "envvars": [
            {
                "variable": "VLLM_EXTRA_ARGS",
                "value": "--max-num-seq 16 --enforce-eager",
            }
        ],
        "enabledAutomaticShmem": false
    },
    {
        "modelName": "Talkativot UI",
        "shortName": "talkativot",
        "environments": {
            "registry": "cr.backend.ai",
            "name": "testing/vllm",
            "tag": "0.6.6-cuda12.4-ubuntu22.04",
            "architecture": "x86_64",
            "environment": "[{'tokenLimit':1024,'vision':false,'apiEndpoint':'https://llama_kor_bllossom.asia03.app.backend.ai','id':'Llama-3.2-Korean-Bllossom-3B','name':'Llama-3.2-Korean-Bllossom-3B'},{'tokenLimit':2048,'vision':false,'apiEndpoint':'https://gemma2_9b.asia03.app.backend.ai/','id':'gemma-2-9b-it','name':'gemma-2-9b-it'},{'tokenLimit':4096,'vision':true,'apiEndpoint':'https://llama-vision.asia03.app.backend.ai/','id':'Llama-3.2-11B-Vision-Instruct','name':'Llama-3.2-11B-Vision-Instruct'}]",
            "version": "1.0.0"
        },
        "runtimeVariant": "vllm",
        "cluster_size": 1,
        "cluster_mode": "single-node",
        "openToPublic": true,
        "resource": {
            "cpu": 4,
            "mem": "32g",
            "accelerator": "10",
            "acceleratorType": "cuda.shares",
            "shmem": "1g"
        },
        "envvars": [],
        "enabledAutomaticShmem": false
    },
    {
        "modelName": "Meta-Llama-3-8B-Instruct",
        "shortName": "llama-3-8b",
        "environments": {
            "registry": "cr.backend.ai",
            "name": "testing/nim",
            "tag": "1.0.0-llama3.8b",
            "architecture": "x86_64",
            "environment": "",
            "version": "1.0.0"
        },
        "runtimeVariant": "nim",
        "cluster_size": 1,
        "cluster_mode": "single-node",
        "openToPublic": true,
        "resource": {
            "cpu": 4,
            "mem": "32g",
            "accelerator": "10",
            "acceleratorType": "cuda.shares",
            "shmem": "1g"
        },
        "envvars": [
            {
                "variable": "NGC_API_KEY",
                "value": "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"
            }
        ],
        "enabledAutomaticShmem": false
    }
]
}